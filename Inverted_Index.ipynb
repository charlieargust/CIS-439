{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "bqGrcfMMjX5_"
      },
      "outputs": [],
      "source": [
        "!pip install -q pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# if letters are combined with numbers, get rid of the numbers"
      ],
      "metadata": {
        "id": "YKM61UJPtp_L"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "conf = SparkConf().setAppName('SparkWordCount') \\\n",
        "                  .set(\"spark.dynamicAllocation.enabled\", \"true\") \\\n",
        "                  .set(\"spark.executor.memory\", \"4g\") \\\n",
        "                  .set(\"spark.executor.cores\", \"2\")\n",
        "\n",
        "sc = SparkContext.getOrCreate(conf=conf)\n",
        "\n",
        "sqlContext = SparkSession.builder \\\n",
        "        .master(\"local\") \\\n",
        "        .appName(\"Colab\") \\\n",
        "        .config('spark.ui.port', '4050') \\\n",
        "        .getOrCreate()"
      ],
      "metadata": {
        "id": "kUhv1YuwkFxU"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "st = LancasterStemmer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1NbwFh4GkHdZ",
        "outputId": "12e8e1b1-50c1-41d2-db29-2449ea77a807"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2w-g6XMakKR0",
        "outputId": "b2991d6e-8280-496c-cc04-145aed6b8947"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_chunks = '/content/drive/MyDrive/Colab Notebooks/wiki2022'\n",
        "\n",
        "text_files_rdd = sc.wholeTextFiles(path_to_chunks)\n",
        "\n",
        "file_names_list = text_files_rdd.keys().map(lambda path: os.path.basename(path)).collect()\n",
        "\n",
        "print(file_names_list)"
      ],
      "metadata": {
        "id": "DsII_K6tswtd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e340e55b-45fd-472d-f3b8-9f64b5b08ad6"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['wiki2022_small.000000', 'wiki2022_small.000002', 'wiki2022_small.000001', 'wiki2022_small.000005', 'wiki2022_small.000004', 'wiki2022_small.000003', 'wiki2022_small.000008', 'wiki2022_small.000007', 'wiki2022_small.000009', 'wiki2022_small.000006', 'wiki2022_small.000010', 'wiki2022_small.000012', 'wiki2022_small.000011', 'wiki2022_small.000014', 'wiki2022_small.000013', 'wiki2022_small.000017', 'wiki2022_small.000015', 'wiki2022_small.000016', 'wiki2022_small.000020', 'wiki2022_small.000019', 'wiki2022_small.000018', 'wiki2022_small.000021', 'wiki2022_small.000022', 'wiki2022_small.000024', 'wiki2022_small.000023', 'wiki2022_small.000025', 'wiki2022_small.000028', 'wiki2022_small.000027', 'wiki2022_small.000026', 'wiki2022_small.000029', 'wiki2022_small.000031', 'wiki2022_small.000030']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "path_to_chunks = '/content/drive/MyDrive/Colab Notebooks/testing_files'\n",
        "\n",
        "text_files_rdd = sc.wholeTextFiles(path_to_chunks)\n",
        "\n",
        "# Extract just the filenames (keys) from RDD, extract basenames, and collect as a list\n",
        "file_names_list = text_files_rdd.keys().map(lambda path: os.path.basename(path)).collect()\n",
        "\n",
        "# Print the list of file names\n",
        "print(file_names_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMs0ethVu3RF",
        "outputId": "b5712f77-e504-404a-fbc2-ea061b2b37f1"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['doc1.txt', 'doc2.txt', 'doc3.txt']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dateutil.parser import parse\n",
        "import datetime\n",
        "import re\n",
        "\n",
        "# getting dates in consistent format\n",
        "\n",
        "def _fix_date(date):\n",
        "  try:\n",
        "    bool(parse(date))\n",
        "    dt = parse(date, default=datetime.datetime(300, 1, 1))\n",
        "    if dt.year != 300:\n",
        "        date = dt.strftime(\"%B %d %Y\")\n",
        "    else:\n",
        "        date = dt.strftime(\"%B %d\")\n",
        "    return date\n",
        "  except:\n",
        "    return date\n",
        "\n",
        "# getting rid of special chararacters\n",
        "\n",
        "def remove_special_chars(word):\n",
        "  if not word.isalpha():\n",
        "     word = re.sub(r'[^\\w\\s]', '', word)\n",
        "  return word\n",
        "\n",
        "# creating document ids\n",
        "\n",
        "def extract_id(doc):\n",
        "  match = re.search(r\"curid=(\\d+)\", doc)\n",
        "  if match:\n",
        "      return match.group(1)\n",
        "  else:\n",
        "      return None\n",
        "\n",
        "# recognizing dates\n",
        "\n",
        "month = r\"([Jj]anuary|[Ff]ebruary|[Mm]arch|[Aa]pril|[Mm]ay|[Jj]une|[Jj]uly|[Aa]ugust|[Ss]eptember|[Oo]ctober|[Nn]ovember|[Dd]ecember)\"\n",
        "day_and_year = r\"\\s(\\d{1,4})(?:st|nd|rd|th)?,?\\s?(\\d{4})?\"\n",
        "#day_and_year = r\"\\s(\\d{1,4}),?\\s?(\\d{4})?\"\n",
        "\n",
        "expression = f'({month}{day_and_year})'\n",
        "\n",
        "slashed_dates = r\"\\b(\\d{1,2})/(0?\\d{1,2}|1[0-2])/(?:\\d{2}|\\d{4})\\b\"\n",
        "slashed_expression = f'({slashed_dates})'\n",
        "\n",
        "date_expression = expression + r\"|\" + slashed_expression\n",
        "\n",
        "# stop words\n",
        "\n",
        "stop_words = stopwords.words(\"english\")\n",
        "stop_words.append('')\n",
        "stop_words.append('also')"
      ],
      "metadata": {
        "id": "-E0LW4iVkOf4"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#input_file = input_file.sample(True, .001)"
      ],
      "metadata": {
        "id": "HFr52_F5kx5X"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_files_rdd.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_ZXqwcwk0wv",
        "outputId": "f36e3ddc-e872-4799-c9bc-daf256e266b7"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('file:/content/drive/MyDrive/Colab Notebooks/testing_files/doc1.txt',\n",
              "  'https://en.wikipedia.org/wiki?curid=1770 my name is charlie and i love cheese                                                                 \\r\\nhttps://en.wikipedia.org/wiki?curid=1773 the wolf jumped over the cheese barn\\r\\nhttps://en.wikipedia.org/wiki?curid=1769 cheese curds'),\n",
              " ('file:/content/drive/MyDrive/Colab Notebooks/testing_files/doc2.txt',\n",
              "  'https://en.wikipedia.org/wiki?curid=1746 the cheese broke down the oregano\\r\\nhttps://en.wikipedia.org/wiki?curid=1892 what is up seasoning with this seasoning'),\n",
              " ('file:/content/drive/MyDrive/Colab Notebooks/testing_files/doc3.txt',\n",
              "  'https://en.wikipedia.org/wiki?curid=124 my lunch is delicious oregano\\r\\nhttps://en.wikipedia.org/wiki?curid=196 the oregano cheese is smelling like oregano')]"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing(doc):\n",
        "  matches = re.findall(date_expression, doc[0])\n",
        "  dates = [match[0] for match in matches if match[0]]\n",
        "\n",
        "  words, docID = doc\n",
        "  print(type(words))\n",
        "  words = words.split()[1:]\n",
        "  for date in dates:\n",
        "    words.append(_fix_date(date))\n",
        "\n",
        "  return [(st.stem(remove_special_chars(word)),\n",
        "           docID,\n",
        "           1) for word in words]"
      ],
      "metadata": {
        "id": "q8SF6_dynGwJ"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def local_inverted_index(input_file):\n",
        "  document = input_file.map(lambda x: (x, extract_id(x)))\n",
        "\n",
        "  docIDs = document.flatMap(lambda doc: preprocessing(doc))\\\n",
        "  .sortBy(lambda x: x)\n",
        "#.filter(lambda x: x[0] not in stop_words)\\\n",
        "#.sortBy(lambda x: x)\n",
        "\n",
        "  combining_docs = docIDs.map(lambda x: ((x[0], x[1]), x[2]))\\\n",
        ".reduceByKey(lambda x, y: x + y)\\\n",
        ".map(lambda x: (x[0][0], x[0][1], x[1]))\n",
        "\n",
        "  inverted_index = combining_docs.map(lambda x: (x[0], [(x[1], x[2])], )) \\\n",
        "            .reduceByKey(lambda x, y: x + y) \\\n",
        "            .map(lambda x: (x[0], x[1]))\n",
        "\n",
        "  return inverted_index"
      ],
      "metadata": {
        "id": "CulrxtCqPts6"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def local_inverted_index(input_file):\n",
        "    # Mapping each line to document ID\n",
        "    document = input_file.map(lambda x: (x, extract_id(x)))\n",
        "\n",
        "    # FlatMapping and sorting documents\n",
        "    docIDs = document.flatMap(lambda doc: preprocessing(doc)).sortBy(lambda x: x)\n",
        "\n",
        "    # Combining documents and counting occurrences\n",
        "    inverted_index = docIDs.map(lambda x: ((x[0], x[1]), x[2])) \\\n",
        "                           .reduceByKey(lambda x, y: x + y) \\\n",
        "                           .map(lambda x: (x[0][0], [(x[0][1], x[1])])) \\\n",
        "                           .reduceByKey(lambda x, y: x + y)\n",
        "\n",
        "    return inverted_index\n"
      ],
      "metadata": {
        "id": "4Xq-zah-YUri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from math import log2\n",
        "\n",
        "def inverse_doc_freq(doc_num, doc_freq):\n",
        "  return log2(doc_num / doc_freq)"
      ],
      "metadata": {
        "id": "wbuiHyc0FmGx"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "path_to_chunks = '/content/drive/MyDrive/Colab Notebooks/testing_files'\n",
        "\n",
        "text_files_rdd = sc.textFile(path_to_chunks + \"/test*\")"
      ],
      "metadata": {
        "id": "Go5Jn-Oh4rWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "shutil.rmtree('index2')"
      ],
      "metadata": {
        "id": "DCdq8TaARjy3"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_rdds = []\n",
        "local_inv_indexes = []\n",
        "\n",
        "for file_name in file_names_list:\n",
        "  input_file = f'{path_to_chunks}/{file_name}'\n",
        "  print(input_file)\n",
        "  input_file = sc.textFile(input_file)\n",
        "\n",
        "  inv_index = local_inverted_index(input_file)\n",
        "\n",
        "  list_of_rdds.append((inv_index, input_file.count()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flE1NI6EKtkc",
        "outputId": "75de9f83-67c8-43c6-8d2b-684fce500699"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/wiki2022/wiki2022_small.000000\n",
            "/content/drive/MyDrive/Colab Notebooks/wiki2022/wiki2022_small.000002\n",
            "/content/drive/MyDrive/Colab Notebooks/wiki2022/wiki2022_small.000001\n",
            "/content/drive/MyDrive/Colab Notebooks/wiki2022/wiki2022_small.000005\n",
            "/content/drive/MyDrive/Colab Notebooks/wiki2022/wiki2022_small.000004\n",
            "/content/drive/MyDrive/Colab Notebooks/wiki2022/wiki2022_small.000003\n",
            "/content/drive/MyDrive/Colab Notebooks/wiki2022/wiki2022_small.000008\n",
            "/content/drive/MyDrive/Colab Notebooks/wiki2022/wiki2022_small.000007\n",
            "/content/drive/MyDrive/Colab Notebooks/wiki2022/wiki2022_small.000009\n",
            "/content/drive/MyDrive/Colab Notebooks/wiki2022/wiki2022_small.000006\n",
            "/content/drive/MyDrive/Colab Notebooks/wiki2022/wiki2022_small.000010\n",
            "/content/drive/MyDrive/Colab Notebooks/wiki2022/wiki2022_small.000012\n",
            "/content/drive/MyDrive/Colab Notebooks/wiki2022/wiki2022_small.000011\n",
            "/content/drive/MyDrive/Colab Notebooks/wiki2022/wiki2022_small.000014\n",
            "/content/drive/MyDrive/Colab Notebooks/wiki2022/wiki2022_small.000013\n",
            "/content/drive/MyDrive/Colab Notebooks/wiki2022/wiki2022_small.000017\n",
            "/content/drive/MyDrive/Colab Notebooks/wiki2022/wiki2022_small.000015\n",
            "/content/drive/MyDrive/Colab Notebooks/wiki2022/wiki2022_small.000016\n",
            "/content/drive/MyDrive/Colab Notebooks/wiki2022/wiki2022_small.000020\n",
            "/content/drive/MyDrive/Colab Notebooks/wiki2022/wiki2022_small.000019\n",
            "/content/drive/MyDrive/Colab Notebooks/wiki2022/wiki2022_small.000018\n",
            "/content/drive/MyDrive/Colab Notebooks/wiki2022/wiki2022_small.000021\n",
            "/content/drive/MyDrive/Colab Notebooks/wiki2022/wiki2022_small.000022\n",
            "/content/drive/MyDrive/Colab Notebooks/wiki2022/wiki2022_small.000024\n",
            "/content/drive/MyDrive/Colab Notebooks/wiki2022/wiki2022_small.000023\n",
            "/content/drive/MyDrive/Colab Notebooks/wiki2022/wiki2022_small.000025\n",
            "/content/drive/MyDrive/Colab Notebooks/wiki2022/wiki2022_small.000028\n",
            "/content/drive/MyDrive/Colab Notebooks/wiki2022/wiki2022_small.000027\n",
            "/content/drive/MyDrive/Colab Notebooks/wiki2022/wiki2022_small.000026\n",
            "/content/drive/MyDrive/Colab Notebooks/wiki2022/wiki2022_small.000029\n",
            "/content/drive/MyDrive/Colab Notebooks/wiki2022/wiki2022_small.000031\n",
            "/content/drive/MyDrive/Colab Notebooks/wiki2022/wiki2022_small.000030\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import concurrent.futures\n",
        "\n",
        "list_of_rdds = []\n",
        "local_inv_indexes = []\n",
        "\n",
        "def process_file(file_name):\n",
        "    input_file = f'{path_to_chunks}/{file_name}'\n",
        "    print(input_file)\n",
        "    input_file = sc.textFile(input_file)\n",
        "    inv_index = local_inverted_index(input_file)\n",
        "    return inv_index, input_file.count()\n",
        "\n",
        "# Process files concurrently using ThreadPoolExecutor\n",
        "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "    futures = [executor.submit(process_file, file_name) for file_name in file_names_list]\n",
        "    for future in concurrent.futures.as_completed(futures):\n",
        "        list_of_rdds.append(future.result())\n",
        "\n",
        "# Now list_of_rdds contains tuples of (inv_index, count) for each file\n"
      ],
      "metadata": {
        "id": "U8dfgImfXUOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_rdds(rdd1, rdd2):\n",
        "  return rdd1.union(rdd2)\n",
        "\n",
        "merged = list_of_rdds[0][0]\n",
        "\n",
        "for i in range(1, len(list_of_rdds)):\n",
        "  merged = merge_rdds(merged, list_of_rdds[i][0])\n",
        "\n",
        "word_codes_rdd = merged.reduceByKey(lambda x, y: x)\\\n",
        ".sortByKey().zipWithIndex().map(lambda x: (x[0][0], x[1]))\n",
        "\n",
        "word_codes_rdd.collect()"
      ],
      "metadata": {
        "id": "0eoe5fc97uiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num = 0\n",
        "\n",
        "for inv_index in list_of_rdds:\n",
        "\n",
        "  doc_count = inv_index[1]\n",
        "\n",
        "  inv_index = inv_index[0].join(word_codes_rdd)\n",
        "\n",
        "  inv_index = inv_index.map(lambda x: (x[1][1],\n",
        "                                       x[0],\n",
        "                                       len(x[1][0]),\n",
        "                                       x[1][0],\n",
        "                                       sum([tf[1] for tf in x[1][0]]),\n",
        "                                       None))\\\n",
        "                                       .map(lambda x: (x[0], x[1], x[2], x[3], x[4],\n",
        "                                                       x[4] * inverse_doc_freq(doc_count, x[2])))\\\n",
        "                                       .sortBy(lambda x: x[5])\n",
        "\n",
        "  inv_index.collect()\n",
        "\n",
        "  #match = re.search(r\"\\.(.*)\", file_name)\n",
        "\n",
        "  #if match:\n",
        "  #    matched_part = match.group(1)\n",
        "\n",
        "\n",
        "\n",
        "  inv_index_single_partition = inv_index.coalesce(1)\n",
        "  output_file_path = 'index' + str(num)\n",
        "  inv_index_single_partition.saveAsTextFile(output_file_path)\n",
        "  num += 1\n",
        "  break"
      ],
      "metadata": {
        "id": "gApwSDc76jGo"
      },
      "execution_count": 104,
      "outputs": []
    }
  ]
}