{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRwb2OjXcOsg"
      },
      "outputs": [],
      "source": [
        "!pip install -q pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bOaK5sgoc-73"
      },
      "outputs": [],
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "conf = SparkConf().setAppName('SparkWordCount')\n",
        "sc = SparkContext.getOrCreate(conf = conf)\n",
        "\n",
        "sqlContext = SparkSession.builder\\\n",
        "        .master(\"local\")\\\n",
        "        .appName(\"Colab\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "st = LancasterStemmer()"
      ],
      "metadata": {
        "id": "QYY-C5FRyblJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24ae34b9-6eef-4646-f0b9-a2f380cb6971"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZphCHsKfIEp"
      },
      "outputs": [],
      "source": [
        "input_file = sc.textFile(\"tiny_wikipedia.txt.gz\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#input_file = input_file.sample(True, .0001)"
      ],
      "metadata": {
        "id": "5xD6QbAuu5P4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dateutil.parser import parse\n",
        "import datetime\n",
        "\n",
        "def _fix_date(date):\n",
        "  try:\n",
        "    bool(parse(date))\n",
        "    dt = parse(date, default=datetime.datetime(300, 1, 1))\n",
        "    if dt.year != 300:\n",
        "        date = dt.strftime(\"%B %d %Y\")\n",
        "    else:\n",
        "        date = dt.strftime(\"%B %d\")\n",
        "    return date\n",
        "  except:\n",
        "    return date"
      ],
      "metadata": {
        "id": "JQF2fel_s1nj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def remove_special_chars(word):\n",
        "  if not word.isalpha():\n",
        "     word = re.sub(r'[^\\w\\s]', '', word)\n",
        "  return word\n",
        "\n",
        "def identify_dates(expression, doc):\n",
        "  matches = re.findall(expression, doc)\n",
        "  dates = [match[0] for match in matches if match[0]]\n",
        "\n",
        "  doc = doc.split()\n",
        "  for date in dates:\n",
        "    doc.append(_fix_date(date))\n",
        "\n",
        "  return doc"
      ],
      "metadata": {
        "id": "BzPB0ASNSU9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# recognizing dates\n",
        "month = r\"([Jj]anuary|[Ff]ebruary|[Mm]arch|[Aa]pril|[Mm]ay|[Jj]une|[Jj]uly|[Aa]ugust|[Ss]eptember|[Oo]ctober|[Nn]ovember|[Dd]ecember)\"\n",
        "day_and_year = r\"\\s(\\d{1,4})(?:st|nd|rd|th)?,?\\s?(\\d{4})?\"\n",
        "#day_and_year = r\"\\s(\\d{1,4}),?\\s?(\\d{4})?\"\n",
        "\n",
        "expression = f'({month}{day_and_year})'\n",
        "\n",
        "slashed_dates = r\"\\b(\\d{1,2})/(0?\\d{1,2}|1[0-2])/(?:\\d{2}|\\d{4})\\b\"\n",
        "slashed_expression = f'({slashed_dates})'\n",
        "\n",
        "date_expression = expression + r\"|\" + slashed_expression\n"
      ],
      "metadata": {
        "id": "Bz160xsCsM5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = stopwords.words(\"english\")\n",
        "stop_words.append('')\n",
        "stop_words.append('also')"
      ],
      "metadata": {
        "id": "LnVRXqROGGCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_stems = input_file.map(lambda line: \\\n",
        " [word_lower for word in identify_dates(date_expression, line)[1:] \\\n",
        "  if (word_lower := remove_special_chars(word.lower())) not in stop_words])\n",
        "\n",
        "all_stems_kv = all_stems.map(lambda doc: [(st.stem(word), 1) for word in doc])"
      ],
      "metadata": {
        "id": "JNPjqRwUt_q-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "global_freq = all_stems_kv.flatMap(lambda word: word)\\\n",
        ".reduceByKey(lambda a, b: a + b)"
      ],
      "metadata": {
        "id": "fGvmGLvpl0tL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_freq = all_stems_kv.map(lambda words: [pair for pair in set(words)])\\\n",
        ".flatMap(lambda word: word)\\\n",
        ".reduceByKey(lambda a, b: a + b)\\\n",
        ".sortBy(lambda x: x[0])"
      ],
      "metadata": {
        "id": "fSJdt9HOMHX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alphabetized = doc_freq.keys().zipWithIndex()"
      ],
      "metadata": {
        "id": "3N09aiyajO9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dictionary = alphabetized.map(lambda x: f\"{x[0]}\")\n",
        "dictionary.saveAsTextFile(\"dictionary.txt\")"
      ],
      "metadata": {
        "id": "pWoIoizKmkS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unigram = alphabetized.join(doc_freq).join(global_freq)"
      ],
      "metadata": {
        "id": "scSnamuxmoha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unigram = unigram.sortBy(lambda word: -word[1][1])\\\n",
        ".map(lambda word: f\"{word[1][0][0]} {word[0]} {word[1][0][1]} {word[1][1]}\")\n",
        "\n",
        "unigram.saveAsTextFile(\"unigrams.txt\")"
      ],
      "metadata": {
        "id": "uCq0pwiJmrVh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}