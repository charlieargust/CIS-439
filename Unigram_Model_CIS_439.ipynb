{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "eRwb2OjXcOsg"
      },
      "outputs": [],
      "source": [
        "!pip install -q pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bOaK5sgoc-73"
      },
      "outputs": [],
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "conf = SparkConf().setAppName('SparkWordCount')\n",
        "sc = SparkContext.getOrCreate(conf = conf)\n",
        "\n",
        "sqlContext = SparkSession.builder\\\n",
        "        .master(\"local\")\\\n",
        "        .appName(\"Colab\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "st = LancasterStemmer()\n",
        "wl = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "QYY-C5FRyblJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b0b9cad-39cd-4c0a-db3d-bbf4b379060b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "FZphCHsKfIEp"
      },
      "outputs": [],
      "source": [
        "input_file = sc.textFile(\"tiny_wikipedia.txt.gz\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#input_file = input_file.sample(withReplacement=False, fraction=0.0001, seed=42)"
      ],
      "metadata": {
        "id": "u_F0mmJvN_nD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#input_file.collect()[:2]"
      ],
      "metadata": {
        "id": "jKs7JdASLdBx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def remove_special_chars(word):\n",
        "  if not word.isalpha():\n",
        "     word = re.sub(r'[^\\w\\s]', '', word)\n",
        "  return word"
      ],
      "metadata": {
        "id": "BzPB0ASNSU9f"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = stopwords.words(\"english\")\n",
        "stop_words.append('')"
      ],
      "metadata": {
        "id": "LnVRXqROGGCq"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_stems = input_file.map(lambda line: \\\n",
        " [word_lower for word in line.split()[1:] \\\n",
        "  if (word_lower := remove_special_chars(word.lower())) not in stop_words])\n",
        "\n",
        "all_stems_kv = all_stems.map(lambda doc: [(st.stem(word), 1) for word in doc])"
      ],
      "metadata": {
        "id": "NrFEsYEUgYSZ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "global_freq = all_stems_kv.flatMap(lambda word: word)\\\n",
        ".reduceByKey(lambda a, b: a + b)"
      ],
      "metadata": {
        "id": "fGvmGLvpl0tL"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alphabetized_glob_freq = global_freq.sortBy(lambda word: word[0])\n",
        "alphabetized_words = [word for word in alphabetized_glob_freq.keys().collect()]\n",
        "\n",
        "with open(\"dictionary.txt\", \"w\") as file:\n",
        "    for word in alphabetized_words:\n",
        "      file.write(word + \"\\n\")"
      ],
      "metadata": {
        "id": "fSJdt9HOMHX9"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decreasing_glob_freq = global_freq.sortBy(lambda count: -count[1])\n",
        "\n",
        "decreasing_tf = [(key, value) for (key, value) in decreasing_glob_freq.collect()]"
      ],
      "metadata": {
        "id": "dMIr-2Z3Xye-"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_freq = all_stems_kv.map(lambda words: [pair for pair in set(words)])\\\n",
        ".flatMap(lambda word: word)\\\n",
        ".reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "#doc_freq.collect()[:10]"
      ],
      "metadata": {
        "id": "3N09aiyajO9U"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "rQhUAav3w1Cu"
      },
      "outputs": [],
      "source": [
        "dictionary = sc.textFile(\"dictionary.txt\")\n",
        "\n",
        "# initializing dictionary that we will store word as key, word count/doc frequency/global frequency as the value\n",
        "dict = {}\n",
        "\n",
        "# for each word, create associated value of order that they are sorted by (alphabetical)\n",
        "#for key, value in decreasing_glob_freq.collect():\n",
        "for pair in decreasing_tf:\n",
        "  dict[pair[0]] = [pair[1]]\n",
        "\n",
        "# old code\n",
        "idx = 0\n",
        "for word in dictionary.collect():\n",
        "  dict[word].append(idx)\n",
        "  idx += 1\n",
        "\n",
        "for key, value in doc_freq.collect():\n",
        "  dict[key].append(value)\n",
        "\n",
        "#print(dict)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"unigrams.txt\", \"w\") as file:\n",
        "  for key in dict.keys():\n",
        "    file.write(f'{dict[key][1]}' + \" \")\n",
        "    file.write(f'|{key}|' + \" \")\n",
        "    file.write(f'{dict[key][2]}' + \" \")\n",
        "    file.write(f'{dict[key][0]}' + \"\\n\")"
      ],
      "metadata": {
        "id": "X_xfMZyozjyK"
      },
      "execution_count": 15,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}