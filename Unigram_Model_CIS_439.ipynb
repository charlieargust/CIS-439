{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "eRwb2OjXcOsg"
      },
      "outputs": [],
      "source": [
        "!pip install -q pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bOaK5sgoc-73"
      },
      "outputs": [],
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "conf = SparkConf().setAppName('SparkWordCount')\n",
        "sc = SparkContext.getOrCreate(conf = conf)\n",
        "\n",
        "sqlContext = SparkSession.builder\\\n",
        "        .master(\"local\")\\\n",
        "        .appName(\"Colab\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "st = LancasterStemmer()"
      ],
      "metadata": {
        "id": "QYY-C5FRyblJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35af80d0-aace-4701-99e6-fb84136c8823"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "FZphCHsKfIEp"
      },
      "outputs": [],
      "source": [
        "input_file = sc.textFile(\"tiny_wikipedia.txt.gz\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#input_file = input_file.sample(True, .0001)"
      ],
      "metadata": {
        "id": "5xD6QbAuu5P4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#input_file.collect()[:2]"
      ],
      "metadata": {
        "id": "jKs7JdASLdBx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dateutil.parser import parse\n",
        "import datetime\n",
        "\n",
        "def _fix_date(date):\n",
        "  try:\n",
        "    bool(parse(date))\n",
        "    dt = parse(date, default=datetime.datetime(300, 1, 1))\n",
        "    if dt.year != 300:\n",
        "        date = dt.strftime(\"%B %d %Y\")\n",
        "    else:\n",
        "        date = dt.strftime(\"%B %d\")\n",
        "    return date\n",
        "  except:\n",
        "    return date"
      ],
      "metadata": {
        "id": "JQF2fel_s1nj"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def remove_special_chars(word):\n",
        "  if not word.isalpha():\n",
        "     word = re.sub(r'[^\\w\\s]', '', word)\n",
        "  return word\n",
        "\n",
        "def identify_dates(expression, doc):\n",
        "  matches = re.findall(expression, doc)\n",
        "  dates = [match[0] for match in matches if match[0]]\n",
        "\n",
        "  doc = doc.split()\n",
        "  for date in dates:\n",
        "    doc.append(_fix_date(date))\n",
        "\n",
        "  return doc"
      ],
      "metadata": {
        "id": "BzPB0ASNSU9f"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# recognizing dates\n",
        "month = r\"([Jj]anuary|[Ff]ebruary|[Mm]arch|[Aa]pril|[Mm]ay|[Jj]une|[Jj]uly|[Aa]ugust|[Ss]eptember|[Oo]ctober|[Nn]ovember|[Dd]ecember)\"\n",
        "day_and_year = r\"\\s(\\d{1,4})(?:st|nd|rd|th)?,?\\s?(\\d{4})?\"\n",
        "#day_and_year = r\"\\s(\\d{1,4}),?\\s?(\\d{4})?\"\n",
        "\n",
        "expression = f'({month}{day_and_year})'\n",
        "\n",
        "slashed_dates = r\"\\b(\\d{1,2})/(0?\\d{1,2}|1[0-2])/(?:\\d{2}|\\d{4})\\b\"\n",
        "slashed_expression = f'({slashed_dates})'\n",
        "\n",
        "date_expression = expression + r\"|\" + slashed_expression\n"
      ],
      "metadata": {
        "id": "Bz160xsCsM5x"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = stopwords.words(\"english\")\n",
        "stop_words.append('')"
      ],
      "metadata": {
        "id": "LnVRXqROGGCq"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_stems = input_file.map(lambda line: \\\n",
        " [word_lower for word in identify_dates(date_expression, line)[1:] \\\n",
        "  if (word_lower := remove_special_chars(word.lower())) not in stop_words])\n",
        "\n",
        "all_stems_kv = all_stems.map(lambda doc: [(st.stem(word), 1) for word in doc])"
      ],
      "metadata": {
        "id": "JNPjqRwUt_q-"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#all_stems = input_file.map(lambda line: \\\n",
        "# [word_lower for word in line.split()[1:] \\\n",
        "#  if (word_lower := remove_special_chars(word.lower())) not in stop_words])\n",
        "\n",
        "#all_stems_kv = all_stems.map(lambda doc: [(st.stem(word), 1) for word in doc])"
      ],
      "metadata": {
        "id": "NrFEsYEUgYSZ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "global_freq = all_stems_kv.flatMap(lambda word: word)\\\n",
        ".reduceByKey(lambda a, b: a + b)"
      ],
      "metadata": {
        "id": "fGvmGLvpl0tL"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alphabetized_glob_freq = global_freq.sortBy(lambda word: word[0])\n",
        "alphabetized_words = [word for word in alphabetized_glob_freq.keys().collect()]\n",
        "\n",
        "with open(\"dictionary.txt\", \"w\") as file:\n",
        "    for word in alphabetized_words:\n",
        "      file.write(word + \"\\n\")"
      ],
      "metadata": {
        "id": "fSJdt9HOMHX9"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decreasing_glob_freq = global_freq.sortBy(lambda count: -count[1])\n",
        "\n",
        "decreasing_tf = [(key, value) for (key, value) in decreasing_glob_freq.collect()]"
      ],
      "metadata": {
        "id": "dMIr-2Z3Xye-"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_freq = all_stems_kv.map(lambda words: [pair for pair in set(words)])\\\n",
        ".flatMap(lambda word: word)\\\n",
        ".reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "doc_freq.cache()"
      ],
      "metadata": {
        "id": "3N09aiyajO9U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b101f3c0-0c46-4341-fc88-955ea362cb72"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PythonRDD[12] at RDD at PythonRDD.scala:53"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "rQhUAav3w1Cu"
      },
      "outputs": [],
      "source": [
        "dictionary = sc.textFile(\"dictionary.txt\")\n",
        "\n",
        "# initializing dictionary that we will store word as key, word count/doc frequency/global frequency as the value\n",
        "dict = {}\n",
        "\n",
        "# for each word, create associated value of order that they are sorted by (alphabetical)\n",
        "#for key, value in decreasing_glob_freq.collect():\n",
        "\n",
        "#for pair in decreasing_tf:\n",
        "#  dict[pair[0]] = [pair[1]]\n",
        "\n",
        "for pair in decreasing_tf:\n",
        "  dict[pair[0]] = [pair[1], None, None]\n",
        "\n",
        "# old code\n",
        "idx = 0\n",
        "for word in dictionary.collect():\n",
        "  dict[word][1] = idx\n",
        "  idx += 1\n",
        "\n",
        "for key, value in doc_freq.collect():\n",
        "  dict[key][2] = value\n",
        "\n",
        "#print(dict)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"unigrams.txt\", \"w\") as file:\n",
        "  for key in dict.keys():\n",
        "    file.write(f'{dict[key][1]}' + \" \")\n",
        "    file.write(f'|{key}|' + \" \")\n",
        "    file.write(f'{dict[key][2]}' + \" \")\n",
        "    file.write(f'{dict[key][0]}' + \"\\n\")"
      ],
      "metadata": {
        "id": "X_xfMZyozjyK"
      },
      "execution_count": 20,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}