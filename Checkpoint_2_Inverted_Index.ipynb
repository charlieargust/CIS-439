{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bqGrcfMMjX5_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba5dcd11-b605-439a-fbf6-164595507d35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "conf = SparkConf().setAppName('InvertedIndex') \\\n",
        "                  .set(\"spark.dynamicAllocation.enabled\", \"true\") \\\n",
        "                  .set(\"spark.executor.memory\", \"40g\") \\\n",
        "                  .set(\"spark.executor.cores\", \"2\")\n",
        "\n",
        "sc = SparkContext.getOrCreate(conf=conf)\n",
        "\n",
        "sqlContext = SparkSession.builder \\\n",
        "        .master(\"local\") \\\n",
        "        .appName(\"Colab\") \\\n",
        "        .config('spark.ui.port', '4050') \\\n",
        "        .getOrCreate()"
      ],
      "metadata": {
        "id": "kUhv1YuwkFxU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1NbwFh4GkHdZ",
        "outputId": "2cfb4936-bc52-4675-dfb7-ce419e62609b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "\n",
        "st = LancasterStemmer()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-4yFIp3WaVc",
        "outputId": "13585761-e84d-45f3-c1ea-d2ce61d1ab07"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "path_to_chunks = '/content/drive/MyDrive/Colab Notebooks/wiki_testing'\n",
        "\n",
        "text_files_rdd = sc.wholeTextFiles(path_to_chunks)\n",
        "\n",
        "file_names_list = text_files_rdd.keys().map(lambda path: os.path.basename(path)).collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FU_bo0jaWc6a",
        "outputId": "8e401450-1282-4bbc-f036-9e0ae1dda846"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['wiki2022_small.000008', 'wiki2022_small.000007', 'wiki2022_small.000009']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dateutil.parser import parse\n",
        "import datetime\n",
        "import re\n",
        "\n",
        "# getting dates in consistent format\n",
        "\n",
        "def _fix_date(date):\n",
        "  try:\n",
        "    bool(parse(date))\n",
        "    dt = parse(date, default=datetime.datetime(300, 1, 1))\n",
        "    if dt.year != 300:\n",
        "        date = dt.strftime(\"%B %d %Y\")\n",
        "    else:\n",
        "        date = dt.strftime(\"%B %d\")\n",
        "    return date\n",
        "  except:\n",
        "    return date\n",
        "\n",
        "# getting rid of special chararacters\n",
        "\n",
        "def remove_special_chars(word):\n",
        "  if not word.isalpha():\n",
        "     word = re.sub(r'[^\\w\\s]', '', word)\n",
        "  return word\n",
        "\n",
        "# creating document ids\n",
        "\n",
        "def extract_id(doc):\n",
        "  matches = re.findall(r\"curid=(\\d+)\\s+(.+?)(?=\\s*https://|\\s*$)\", doc[1])\n",
        "  return [(match[1].strip(), match[0], doc[0]) for match in matches]\n",
        "\n",
        "# chunk id\n",
        "\n",
        "def extract_chunk_id(file_path):\n",
        "  return int(file_path[-2:])\n",
        "\n",
        "# recognizing dates\n",
        "\n",
        "month = r\"([Jj]anuary|[Ff]ebruary|[Mm]arch|[Aa]pril|[Mm]ay|[Jj]une|[Jj]uly|[Aa]ugust|[Ss]eptember|[Oo]ctober|[Nn]ovember|[Dd]ecember)\"\n",
        "day_and_year = r\"\\s(\\d{1,4})(?:st|nd|rd|th)?,?\\s?(\\d{4})?\"\n",
        "#day_and_year = r\"\\s(\\d{1,4}),?\\s?(\\d{4})?\"\n",
        "\n",
        "expression = f'({month}{day_and_year})'\n",
        "\n",
        "slashed_dates = r\"\\b(\\d{1,2})/(0?\\d{1,2}|1[0-2])/(?:\\d{2}|\\d{4})\\b\"\n",
        "slashed_expression = f'({slashed_dates})'\n",
        "\n",
        "date_expression = expression + r\"|\" + slashed_expression\n",
        "\n",
        "# doc freq\n",
        "\n",
        "from math import log2\n",
        "\n",
        "def inverse_doc_freq(doc_num, doc_freq):\n",
        "  return log2(doc_num / doc_freq)"
      ],
      "metadata": {
        "id": "iUTNqMHJWfVf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing(doc):\n",
        "  matches = re.findall(date_expression, doc[0])\n",
        "  dates = [match[0] for match in matches if match[0]]\n",
        "\n",
        "  words, docID, chunk = doc\n",
        "  words = words.split()[1:]\n",
        "  for date in dates:\n",
        "    words.append(_fix_date(date))\n",
        "\n",
        "  return [((st.stem(remove_special_chars(word)),\n",
        "           docID,\n",
        "            chunk),\n",
        "           1) for word in words]"
      ],
      "metadata": {
        "id": "zYz-jCDgWz_R"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def local_inverted_index(input_file):\n",
        "    document = text_files_rdd.map(lambda x: (extract_chunk_id(x[0]), x[1]))\\\n",
        "    .flatMap(lambda x: extract_id(x))\n",
        "\n",
        "    docIDs = document.flatMap(lambda doc: preprocessing(doc)).reduceByKey(lambda x, y: x + y)\\\n",
        ".map(lambda x: (x[0][0], x[0][1], x[1], x[0][2])).sortBy(lambda x: x)\\\n",
        ".filter(lambda x: x[0] != '')\n",
        "\n",
        "    inverted_index = docIDs.map(lambda x: ((x[0], x[1], x[3]), x[2])) \\\n",
        "                           .reduceByKey(lambda x, y: x + y) \\\n",
        "                           .map(lambda x: ((x[0][0],x[0][2]), [(x[0][1], x[1])])) \\\n",
        "                           .reduceByKey(lambda x, y: x + y)\\\n",
        "                           .sortBy(lambda x: x)\\\n",
        "                           .map(lambda x: (x[0][0], len(x[1]), x[1], x[0][1]))\\\n",
        "                           .filter(lambda x: x[1] > 1)\n",
        "    return inverted_index"
      ],
      "metadata": {
        "id": "qrr6z-4wbbZ3"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "without_word_codes = local_inverted_index(text_files_rdd)\n",
        "\n",
        "filtered_rdd = without_word_codes.filter(lambda x: x[3] == 7)\n",
        "\n",
        "doc_num = filtered_rdd.filter(lambda x: x[0] == 'the').collect()[0][1]\n",
        "\n",
        "filtered_rdd = filtered_rdd.map(lambda x: (doc_num, sum([term[1] for term in x[2]]), x[1],\n",
        "                                        x[0]))\\\n",
        "                        .map(lambda x: (x[3], inverse_doc_freq(x[0], x[2])))\\\n",
        "                        .filter(lambda x: x[1] <= 1)\n",
        "\n",
        "unimportant_words = filtered_rdd.keys().collect()"
      ],
      "metadata": {
        "id": "zl7Ji_m7kT56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "without_word_codes = without_word_codes.filter(lambda x: x[0] not in unimportant_words)\n",
        "\n",
        "arr = np.unique(without_word_codes.keys().collect())\n",
        "\n",
        "word_codes_dict = {element: index for index, element in enumerate(arr)}\n",
        "\n",
        "joined_rdd = without_word_codes.map(lambda x: (x[0], (x[1:], word_codes_dict.get(x[0]))))\n",
        "\n",
        "final_rdd = joined_rdd.map(lambda x: (x[1][1], x[0], x[1][0][0], x[1][0][1], x[1][0][2]))"
      ],
      "metadata": {
        "id": "ilSquFVndEdQ"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "array_str = '\\n'.join(arr)\n",
        "\n",
        "with open('dictionary.txt', 'w') as f:\n",
        "    f.write(array_str)"
      ],
      "metadata": {
        "id": "w11ReeDtsFhv"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def produce_output_file(file_path, rdd):\n",
        "  chunk_num = extract_chunk_id(file_path)\n",
        "\n",
        "  filtered_rdd = rdd.filter(lambda x: x[4] == chunk_num)\\\n",
        "  .map(lambda x: f'{x[0]} {x[1]} {x[2]} {x[3]}')\n",
        "\n",
        "  filtered_rdd = filtered_rdd.coalesce(1)\n",
        "  filtered_rdd.saveAsTextFile(f'index{chunk_num}')"
      ],
      "metadata": {
        "id": "ritI7OZWii_i"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for file_name in file_names_list:\n",
        "  produce_output_file(file_name, final_rdd)"
      ],
      "metadata": {
        "id": "2ANrkerKskKv"
      },
      "execution_count": 122,
      "outputs": []
    }
  ]
}